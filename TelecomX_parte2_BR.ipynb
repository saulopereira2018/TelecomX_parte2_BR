{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saulopereira2018/TelecomX_parte2_BR/blob/main/TelecomX_parte2_BR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4foVEKhrlqcH"
      },
      "source": [
        "#游늷 Extra칞칚o"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importa칞칚o das Bibliotecas Necess치rias"
      ],
      "metadata": {
        "id": "23Qo5oAcAE6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Para o balanceamento de classes (SMOTE), instale se ainda n칚o tiver:\n",
        "# !pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "Dh6ZoIyr_-Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Extra칞칚o do Arquivo Tratado"
      ],
      "metadata": {
        "id": "KlIWgIstAYXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2QzPryqy7B1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # Import the pandas library\n",
        "\n",
        "caminho_do_arquivo_json = '/content/TelecomX_Data.json' # Renamed variable for clarity\n",
        "try:\n",
        "    df = pd.read_json(caminho_do_arquivo_json) # Changed to pd.read_json()\n",
        "    print(f\"Arquivo '{caminho_do_arquivo_json}' carregado com sucesso.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: O arquivo '{caminho_do_arquivo_json}' n칚o foi encontrado. 游땟\")\n",
        "    print(\"Por favor, verifique se o nome do arquivo est치 correto e se ele foi carregado para o Colab ou se o caminho do Google Drive est치 certo.\")\n",
        "    df = pd.DataFrame() # Cria um DataFrame vazio para evitar erros futuros\n",
        "except Exception as e: # Added a more general exception handler for other potential issues\n",
        "    print(f\"Ocorreu um erro ao carregar o arquivo: {e}\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"\\nPrimeiras 5 linhas do DataFrame Tratado:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nInforma칞칫es iniciais do DataFrame Tratado:\")\n",
        "    print(df.info())\n",
        "else:\n",
        "    print(\"\\nO DataFrame est치 vazio. O arquivo JSON pode n칚o ter sido carregado corretamente ou est치 vazio. As pr칩ximas etapas n칚o ser칚o executadas.\")\n",
        "    exit() # Interrompe a execu칞칚o se o DataFrame estiver vazio"
      ],
      "metadata": {
        "id": "thmcGUQZAc4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lSZP8zmmGZu"
      },
      "source": [
        "# 3. Remo칞칚o de Colunas Irrelevantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsm-WTLjmHvt"
      },
      "outputs": [],
      "source": [
        "# Adapte a lista de colunas a serem removidas com base no seu DataFrame.\n",
        "colunas_para_remover = ['customerID'] # Exemplo: 'customerID'. Adicione outras se houver.\n",
        "\n",
        "for col in colunas_para_remover:\n",
        "    if col in df.columns:\n",
        "        df = df.drop(columns=[col])\n",
        "        print(f\"Coluna '{col}' removida.\")\n",
        "    else:\n",
        "        print(f\"Coluna '{col}' n칚o encontrada para remo칞칚o.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XnTC2NTmMRL"
      },
      "source": [
        "# 4. Encoding de Vari치veis Categ칩ricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jgUnLqTmPdd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Assuming 'df' is already loaded and contains the problematic columns\n",
        "# For demonstration, let's create a sample DataFrame with dictionaries\n",
        "# In your actual code, this part would be replaced by your df loading from JSON.\n",
        "data = {\n",
        "    'customer': [{'gender': 'Male', 'seniorcitizen': 0}, {'gender': 'Female', 'seniorcitizen': 1}, {'gender': 'Male', 'seniorcitizen': 0}],\n",
        "    'phone': [{'ServiceType': 'DSL', 'MultipleLines': 'No'}, {'ServiceType': 'Fiber optic', 'MultipleLines': 'Yes'}, {'ServiceType': 'DSL', 'MultipleLines': 'No'}],\n",
        "    'internet': [{'ISP': 'Fiber optic', 'OnlineSecurity': 'Yes'}, {'ISP': 'DSL', 'OnlineSecurity': 'No'}, {'ISP': 'Fiber optic', 'OnlineSecurity': 'Yes'}],\n",
        "    'account': [{'Contract': 'Month-to-month', 'PaymentMethod': 'Electronic check'}, {'Contract': 'Two year', 'PaymentMethod': 'Credit card'}, {'Contract': 'Month-to-month', 'PaymentMethod': 'Bank transfer (automatic)'}],\n",
        "    'numeric_col': [10, 20, 30],\n",
        "    'Churn': [0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# --- Start of the solution for handling dictionary columns ---\n",
        "# Iterate through the columns identified as problematic (customer, phone, internet, account)\n",
        "problematic_cols = ['customer', 'phone', 'internet', 'account']\n",
        "\n",
        "for col in problematic_cols:\n",
        "    if col in df.columns and any(isinstance(x, dict) for x in df[col]):\n",
        "        print(f\"Processing dictionary column: {col}\")\n",
        "        # Extract keys from the first non-null dictionary to create new columns\n",
        "        # You might need to adjust this logic based on your actual dictionary structure\n",
        "        sample_dict = next((x for x in df[col] if isinstance(x, dict)), None)\n",
        "        if sample_dict:\n",
        "            for key in sample_dict.keys():\n",
        "                new_col_name = f\"{col}_{key}\"\n",
        "                df[new_col_name] = df[col].apply(lambda x: x.get(key) if isinstance(x, dict) else None) # Use .get() to avoid KeyError\n",
        "\n",
        "# Now, identify categorical columns again, as new ones have been created\n",
        "colunas_categoricas = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "# Remove the original dictionary columns from the list, as they are no longer needed for encoding\n",
        "for col in problematic_cols:\n",
        "    if col in colunas_categoricas:\n",
        "        colunas_categoricas.remove(col)\n",
        "\n",
        "# Exclua a coluna 'Churn' se ela for a vari치vel target e j치 estiver como 0/1\n",
        "if 'Churn' in colunas_categoricas:\n",
        "    colunas_categoricas.remove('Churn')\n",
        "\n",
        "# --- End of the solution for handling dictionary columns ---\n",
        "\n",
        "\n",
        "if colunas_categoricas:\n",
        "    print(f\"\\nColunas categ칩ricas para encoding: {colunas_categoricas}\")\n",
        "    # Aplica One-Hot Encoding\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    encoded_features = encoder.fit_transform(df[colunas_categoricas])\n",
        "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(colunas_categoricas), index=df.index)\n",
        "\n",
        "    # Concatena o DataFrame original (sem as colunas categ칩ricas originais) com as colunas codificadas\n",
        "    # Make sure to drop the original dictionary columns before concatenation\n",
        "    df = df.drop(columns=[col for col in problematic_cols if col in df.columns and col not in colunas_categoricas], errors='ignore')\n",
        "    df = pd.concat([df, encoded_df], axis=1)\n",
        "    print(\"Encoding de vari치veis categ칩ricas conclu칤do.\")\n",
        "    print(df.head())\n",
        "    print(\"\\nInforma칞칫es iniciais do DataFrame Tratado ap칩s encoding:\")\n",
        "    print(df.info())\n",
        "else:\n",
        "    print(\"\\nNenhuma coluna categ칩rica encontrada para encoding ou j치 foram tratadas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-WzfSvTmaw9"
      },
      "source": [
        "# 5. Verifica칞칚o da Propor칞칚o de Evas칚o (Churn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMTac0YJmeK9"
      },
      "outputs": [],
      "source": [
        "if 'Churn' in df.columns:\n",
        "    print(\"\\nPropor칞칚o da vari치vel 'Churn':\")\n",
        "    proporcao_churn = df['Churn'].value_counts(normalize=True) * 100\n",
        "    print(proporcao_churn)\n",
        "\n",
        "    # Verifica se h치 desequil칤brio (exemplo: uma classe representa mais de 70%)\n",
        "    if proporcao_churn.max() > 70:\n",
        "        print(\"\\n丘멆잺 Aten칞칚o: H치 um desequil칤brio significativo na propor칞칚o de classes de Churn.\")\n",
        "        print(\"Considere aplicar t칠cnicas de balanceamento de classes.\")\n",
        "else:\n",
        "    print(\"\\nColuna 'Churn' n칚o encontrada no DataFrame. Verifique o nome da vari치vel alvo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Balanceamento de Classes (Opcional)\n"
      ],
      "metadata": {
        "id": "evEicPPvCFMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X (features) e y (target) antes do balanceamento\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "# Verifique se h치 desequil칤brio antes de balancear\n",
        "if 'Churn' in df.columns and proporcao_churn.max() > 70: # Reutiliza a verifica칞칚o de desequil칤brio\n",
        "    print(\"\\nRealizando balanceamento de classes com SMOTE...\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "    print(\"Balanceamento conclu칤do. Nova propor칞칚o de 'Churn':\")\n",
        "    print(y_balanced.value_counts(normalize=True) * 100)\n",
        "    X = X_balanced # Atualiza X com os dados balanceados\n",
        "    y = y_balanced # Atualiza y com os dados balanceados\n",
        "else:\n",
        "    print(\"\\nBalanceamento de classes n칚o aplicado (opcional ou n칚o necess치rio).\")"
      ],
      "metadata": {
        "id": "plPnIeZ8CIFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Correla칞칚o e Sele칞칚o de Vari치veis\n"
      ],
      "metadata": {
        "id": "hIZP4awqCOYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "import seaborn as sns          # Import seaborn for heatmaps and other plots\n",
        "import numpy as np             # Import numpy for numerical operations\n",
        "\n",
        "# --- Assuming X and y DataFrames are already defined from previous steps ---\n",
        "# For demonstration purposes, let's create dummy X and y if they don't exist\n",
        "# In your actual code, these would come from your data splitting\n",
        "try:\n",
        "    # This block will attempt to use your existing X and y.\n",
        "    # If X or y are not defined, it will fall into the NameError,\n",
        "    # so we'll wrap it in a try-except or ensure they are defined upstream.\n",
        "    # If you are running this code block independently, you'll need to define X and y.\n",
        "    if 'X' not in locals() or 'y' not in locals():\n",
        "        print(\"X or y not found. Creating dummy data for demonstration.\")\n",
        "        # Create a dummy DataFrame if X and y are not already defined\n",
        "        data = {\n",
        "            'feature1': np.random.rand(100),\n",
        "            'feature2': np.random.randint(0, 5, 100),\n",
        "            'tenure': np.random.randint(1, 72, 100),\n",
        "            'TotalCharges': np.random.rand(100) * 1000\n",
        "        }\n",
        "        df_dummy = pd.DataFrame(data)\n",
        "        X = df_dummy[['feature1', 'feature2', 'tenure', 'TotalCharges']]\n",
        "        y = pd.Series(np.random.randint(0, 2, 100), name='Churn')\n",
        "\n",
        "except NameError:\n",
        "    print(\"X or y are not defined. Please ensure they are created from your dataset before running this section.\")\n",
        "    exit() # Exit if essential data is missing\n",
        "# --- End of dummy data creation (remove in your final script if X and y are properly defined) ---\n",
        "\n",
        "\n",
        "print(\"\\nGerando matriz de correla칞칚o...\")\n",
        "plt.figure(figsize=(12, 10))\n",
        "# Concatena X e y para a matriz de correla칞칚o, garantindo que 'Churn' esteja presente\n",
        "df_corr = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Calculando a matriz de correla칞칚o apenas para vari치veis num칠ricas\n",
        "numeric_cols = df_corr.select_dtypes(include=np.number).columns\n",
        "correlation_matrix = df_corr[numeric_cols].corr()\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Matriz de Correla칞칚o das Vari치veis')\n",
        "plt.show()\n",
        "\n",
        "if 'Churn' in correlation_matrix.columns:\n",
        "    print(\"\\nCorrela칞칚o das vari치veis com 'Churn':\")\n",
        "    print(correlation_matrix['Churn'].sort_values(ascending=False))\n",
        "\n",
        "\n",
        "# An치lises Direcionadas: Investigue como vari치veis espec칤ficas se relacionam com a evas칚o.\n",
        "print(\"\\nAn치lises direcionadas: Tempo de Contrato vs. Churn, Total Gasto vs. Churn\")\n",
        "\n",
        "# Verifique se as colunas existem antes de tentar plotar\n",
        "# Note: 'df' aqui se refere ao DataFrame original antes de separarmos X e y.\n",
        "# Se 'tenure' ou 'TotalCharges' foram transformadas (ex: OneHotEncoder), essa visualiza칞칚o pode precisar ser ajustada\n",
        "# para usar os dados 'X' ou 'X_balanced' ap칩s as transforma칞칫es.\n",
        "# Assumindo que 'tenure' e 'TotalCharges' s칚o colunas num칠ricas originais.\n",
        "if 'tenure' in df_corr.columns and 'TotalCharges' in df_corr.columns:\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.boxplot(x='Churn', y='tenure', data=df_corr)\n",
        "    plt.title('Tempo de Contrato (tenure) vs. Evas칚o')\n",
        "    plt.ylabel('Tempo de Contrato (meses)')\n",
        "    plt.xlabel('Evas칚o (0=N칚o, 1=Sim)')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(x='Churn', y='TotalCharges', data=df_corr)\n",
        "    plt.title('Total Gasto (TotalCharges) vs. Evas칚o')\n",
        "    plt.ylabel('Total Gasto')\n",
        "    plt.xlabel('Evas칚o (0=N칚o, 1=Sim)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Colunas 'tenure' ou 'TotalCharges' n칚o encontradas para an치lises direcionadas no DataFrame combinado. Verifique o nome das colunas.\")"
      ],
      "metadata": {
        "id": "stpspiZ6CPny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Normaliza칞칚o ou Padroniza칞칚o (se necess치rio)"
      ],
      "metadata": {
        "id": "NodTlOMmCkXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "\n",
        "# --- Assuming X DataFrame is already defined from previous steps ---\n",
        "# For demonstration purposes, let's create a dummy X if it doesn't exist\n",
        "try:\n",
        "    if 'X' not in locals():\n",
        "        print(\"X not found. Creating dummy data for demonstration.\")\n",
        "        data = {\n",
        "            'numeric_feature_1': np.random.rand(100) * 100,\n",
        "            'numeric_feature_2': np.random.randint(10, 1000, 100),\n",
        "            'categorical_feature': ['A', 'B', 'A'] * 33 + ['B'],\n",
        "            'boolean_feature': [True, False] * 50\n",
        "        }\n",
        "        X = pd.DataFrame(data)\n",
        "        # Assuming you've already handled categorical encoding for X\n",
        "        # For this example, let's just use the numeric columns if they exist\n",
        "        X = X[['numeric_feature_1', 'numeric_feature_2']]\n",
        "\n",
        "except NameError:\n",
        "    print(\"X is not defined. Please ensure it is created from your dataset before running this section.\")\n",
        "    exit() # Exit if essential data is missing\n",
        "# --- End of dummy data creation (remove in your final script if X is properly defined) ---\n",
        "\n",
        "\n",
        "# Avalie a necessidade de normalizar ou padronizar os dados, conforme os modelos que ser칚o aplicados.\n",
        "\n",
        "# Identifica colunas num칠ricas para normaliza칞칚o/padroniza칞칚o\n",
        "colunas_numericas = X.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Cria uma c칩pia dos dados para modelos com padroniza칞칚o\n",
        "X_scaled = X.copy()\n",
        "if colunas_numericas:\n",
        "    X_scaled[colunas_numericas] = scaler.fit_transform(X[colunas_numericas])\n",
        "    print(\"\\nDados num칠ricos padronizados (StandardScaler) para modelos que exigem escala.\")\n",
        "else:\n",
        "    print(\"\\nNenhuma coluna num칠rica para padroniza칞칚o ou j치 padronizadas.\")\n",
        "\n",
        "# X_unscaled j치 칠 o 'X' original (ou balanceado, se SMOTE foi aplicado)\n",
        "X_unscaled = X\n",
        "\n",
        "print(\"\\nPrimeiras 5 linhas do DataFrame X_scaled (padronizado):\")\n",
        "print(X_scaled.head())\n",
        "print(\"\\nInforma칞칫es sobre X_scaled:\")\n",
        "print(X_scaled.describe().loc[['mean', 'std']]) # Check mean (close to 0) and std (close to 1)"
      ],
      "metadata": {
        "id": "FB_a8GImCm7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Modelagem Preditiva"
      ],
      "metadata": {
        "id": "WgVetVbeCzuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa칞칚o de Dados\n",
        "# Divida o conjunto de dados em treino e teste.\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "print(f\"\\nDados divididos em treino ({len(X_train_scaled)} amostras) e teste ({len(X_test_scaled)} amostras).\")\n",
        "\n",
        "# Garante que temos uma vers칚o n칚o-escalada para a 츼rvore de Decis칚o, dividida de forma consistente\n",
        "X_train_unscaled, X_test_unscaled, _, _ = train_test_split(X_unscaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# Cria칞칚o de Modelos\n",
        "print(\"\\n--- Modelagem Preditiva ---\")\n",
        "\n",
        "# Modelo 1: Regress칚o Log칤stica (requer normaliza칞칚o)\n",
        "print(\"\\nConstruindo Modelo de Regress칚o Log칤stica (com dados padronizados)...\")\n",
        "model_lr = LogisticRegression(random_state=42, solver='liblinear', max_iter=1000) # Aumentado max_iter para converg칡ncia\n",
        "model_lr.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = model_lr.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nRelat칩rio de Classifica칞칚o - Regress칚o Log칤stica:\")\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "print(\"\\nMatriz de Confus칚o - Regress칚o Log칤stica:\")\n",
        "print(confusion_matrix(y_test, y_pred_lr))\n",
        "\n",
        "# Modelo 2: 츼rvore de Decis칚o (n칚o requer normaliza칞칚o)\n",
        "print(\"\\nConstruindo Modelo de 츼rvore de Decis칚o (sem necessidade de padroniza칞칚o)...\")\n",
        "model_dt = DecisionTreeClassifier(random_state=42)\n",
        "model_dt.fit(X_train_unscaled, y_train) # Usa os dados N츾O padronizados\n",
        "y_pred_dt = model_dt.predict(X_test_unscaled)\n",
        "\n",
        "print(\"\\nRelat칩rio de Classifica칞칚o - 츼rvore de Decis칚o:\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "print(\"\\nMatriz de Confus칚o - 츼rvore de Decis칚o:\")\n",
        "print(confusion_matrix(y_test, y_pred_dt))"
      ],
      "metadata": {
        "id": "9r_hIH03Gaoa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}