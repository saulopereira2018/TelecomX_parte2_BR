{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saulopereira2018/TelecomX_parte2_BR/blob/main/TelecomX_parte2_BR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4foVEKhrlqcH"
      },
      "source": [
        "#üìå Extra√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importa√ß√£o das Bibliotecas Necess√°rias"
      ],
      "metadata": {
        "id": "23Qo5oAcAE6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Para o balanceamento de classes (SMOTE), instale se ainda n√£o tiver:\n",
        "# !pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "Dh6ZoIyr_-Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Extra√ß√£o do Arquivo Tratado"
      ],
      "metadata": {
        "id": "KlIWgIstAYXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2QzPryqy7B1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # Import the pandas library\n",
        "\n",
        "caminho_do_arquivo_json = '/content/TelecomX_Data.json' # Renamed variable for clarity\n",
        "try:\n",
        "    df = pd.read_json(caminho_do_arquivo_json) # Changed to pd.read_json()\n",
        "    print(f\"Arquivo '{caminho_do_arquivo_json}' carregado com sucesso.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: O arquivo '{caminho_do_arquivo_json}' n√£o foi encontrado. üòï\")\n",
        "    print(\"Por favor, verifique se o nome do arquivo est√° correto e se ele foi carregado para o Colab ou se o caminho do Google Drive est√° certo.\")\n",
        "    df = pd.DataFrame() # Cria um DataFrame vazio para evitar erros futuros\n",
        "except Exception as e: # Added a more general exception handler for other potential issues\n",
        "    print(f\"Ocorreu um erro ao carregar o arquivo: {e}\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"\\nPrimeiras 5 linhas do DataFrame Tratado:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nInforma√ß√µes iniciais do DataFrame Tratado:\")\n",
        "    print(df.info())\n",
        "else:\n",
        "    print(\"\\nO DataFrame est√° vazio. O arquivo JSON pode n√£o ter sido carregado corretamente ou est√° vazio. As pr√≥ximas etapas n√£o ser√£o executadas.\")\n",
        "    exit() # Interrompe a execu√ß√£o se o DataFrame estiver vazio"
      ],
      "metadata": {
        "id": "thmcGUQZAc4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lSZP8zmmGZu"
      },
      "source": [
        "# 3. Remo√ß√£o de Colunas Irrelevantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsm-WTLjmHvt"
      },
      "outputs": [],
      "source": [
        "# Adapte a lista de colunas a serem removidas com base no seu DataFrame.\n",
        "colunas_para_remover = ['customerID'] # Exemplo: 'customerID'. Adicione outras se houver.\n",
        "\n",
        "for col in colunas_para_remover:\n",
        "    if col in df.columns:\n",
        "        df = df.drop(columns=[col])\n",
        "        print(f\"Coluna '{col}' removida.\")\n",
        "    else:\n",
        "        print(f\"Coluna '{col}' n√£o encontrada para remo√ß√£o.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XnTC2NTmMRL"
      },
      "source": [
        "# 4. Encoding de Vari√°veis Categ√≥ricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jgUnLqTmPdd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Assuming 'df' is already loaded and contains the problematic columns\n",
        "# For demonstration, let's create a sample DataFrame with dictionaries\n",
        "# In your actual code, this part would be replaced by your df loading from JSON.\n",
        "data = {\n",
        "    'customer': [{'gender': 'Male', 'seniorcitizen': 0}, {'gender': 'Female', 'seniorcitizen': 1}, {'gender': 'Male', 'seniorcitizen': 0}],\n",
        "    'phone': [{'ServiceType': 'DSL', 'MultipleLines': 'No'}, {'ServiceType': 'Fiber optic', 'MultipleLines': 'Yes'}, {'ServiceType': 'DSL', 'MultipleLines': 'No'}],\n",
        "    'internet': [{'ISP': 'Fiber optic', 'OnlineSecurity': 'Yes'}, {'ISP': 'DSL', 'OnlineSecurity': 'No'}, {'ISP': 'Fiber optic', 'OnlineSecurity': 'Yes'}],\n",
        "    'account': [{'Contract': 'Month-to-month', 'PaymentMethod': 'Electronic check'}, {'Contract': 'Two year', 'PaymentMethod': 'Credit card'}, {'Contract': 'Month-to-month', 'PaymentMethod': 'Bank transfer (automatic)'}],\n",
        "    'numeric_col': [10, 20, 30],\n",
        "    'Churn': [0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# --- Start of the solution for handling dictionary columns ---\n",
        "# Iterate through the columns identified as problematic (customer, phone, internet, account)\n",
        "problematic_cols = ['customer', 'phone', 'internet', 'account']\n",
        "\n",
        "for col in problematic_cols:\n",
        "    if col in df.columns and any(isinstance(x, dict) for x in df[col]):\n",
        "        print(f\"Processing dictionary column: {col}\")\n",
        "        # Extract keys from the first non-null dictionary to create new columns\n",
        "        # You might need to adjust this logic based on your actual dictionary structure\n",
        "        sample_dict = next((x for x in df[col] if isinstance(x, dict)), None)\n",
        "        if sample_dict:\n",
        "            for key in sample_dict.keys():\n",
        "                new_col_name = f\"{col}_{key}\"\n",
        "                df[new_col_name] = df[col].apply(lambda x: x.get(key) if isinstance(x, dict) else None) # Use .get() to avoid KeyError\n",
        "\n",
        "# Now, identify categorical columns again, as new ones have been created\n",
        "colunas_categoricas = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "# Remove the original dictionary columns from the list, as they are no longer needed for encoding\n",
        "for col in problematic_cols:\n",
        "    if col in colunas_categoricas:\n",
        "        colunas_categoricas.remove(col)\n",
        "\n",
        "# Exclua a coluna 'Churn' se ela for a vari√°vel target e j√° estiver como 0/1\n",
        "if 'Churn' in colunas_categoricas:\n",
        "    colunas_categoricas.remove('Churn')\n",
        "\n",
        "# --- End of the solution for handling dictionary columns ---\n",
        "\n",
        "\n",
        "if colunas_categoricas:\n",
        "    print(f\"\\nColunas categ√≥ricas para encoding: {colunas_categoricas}\")\n",
        "    # Aplica One-Hot Encoding\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    encoded_features = encoder.fit_transform(df[colunas_categoricas])\n",
        "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(colunas_categoricas), index=df.index)\n",
        "\n",
        "    # Concatena o DataFrame original (sem as colunas categ√≥ricas originais) com as colunas codificadas\n",
        "    # Make sure to drop the original dictionary columns before concatenation\n",
        "    df = df.drop(columns=[col for col in problematic_cols if col in df.columns and col not in colunas_categoricas], errors='ignore')\n",
        "    df = pd.concat([df, encoded_df], axis=1)\n",
        "    print(\"Encoding de vari√°veis categ√≥ricas conclu√≠do.\")\n",
        "    print(df.head())\n",
        "    print(\"\\nInforma√ß√µes iniciais do DataFrame Tratado ap√≥s encoding:\")\n",
        "    print(df.info())\n",
        "else:\n",
        "    print(\"\\nNenhuma coluna categ√≥rica encontrada para encoding ou j√° foram tratadas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-WzfSvTmaw9"
      },
      "source": [
        "# 5. Verifica√ß√£o da Propor√ß√£o de Evas√£o (Churn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMTac0YJmeK9"
      },
      "outputs": [],
      "source": [
        "if 'Churn' in df.columns:\n",
        "    print(\"\\nPropor√ß√£o da vari√°vel 'Churn':\")\n",
        "    proporcao_churn = df['Churn'].value_counts(normalize=True) * 100\n",
        "    print(proporcao_churn)\n",
        "\n",
        "    # Verifica se h√° desequil√≠brio (exemplo: uma classe representa mais de 70%)\n",
        "    if proporcao_churn.max() > 70:\n",
        "        print(\"\\n‚ö†Ô∏è Aten√ß√£o: H√° um desequil√≠brio significativo na propor√ß√£o de classes de Churn.\")\n",
        "        print(\"Considere aplicar t√©cnicas de balanceamento de classes.\")\n",
        "else:\n",
        "    print(\"\\nColuna 'Churn' n√£o encontrada no DataFrame. Verifique o nome da vari√°vel alvo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Balanceamento de Classes (Opcional)\n"
      ],
      "metadata": {
        "id": "evEicPPvCFMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X (features) e y (target) antes do balanceamento\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "# Verifique se h√° desequil√≠brio antes de balancear\n",
        "if 'Churn' in df.columns and proporcao_churn.max() > 70: # Reutiliza a verifica√ß√£o de desequil√≠brio\n",
        "    print(\"\\nRealizando balanceamento de classes com SMOTE...\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "    print(\"Balanceamento conclu√≠do. Nova propor√ß√£o de 'Churn':\")\n",
        "    print(y_balanced.value_counts(normalize=True) * 100)\n",
        "    X = X_balanced # Atualiza X com os dados balanceados\n",
        "    y = y_balanced # Atualiza y com os dados balanceados\n",
        "else:\n",
        "    print(\"\\nBalanceamento de classes n√£o aplicado (opcional ou n√£o necess√°rio).\")"
      ],
      "metadata": {
        "id": "plPnIeZ8CIFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Correla√ß√£o e Sele√ß√£o de Vari√°veis\n"
      ],
      "metadata": {
        "id": "hIZP4awqCOYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "import seaborn as sns          # Import seaborn for heatmaps and other plots\n",
        "import numpy as np             # Import numpy for numerical operations\n",
        "\n",
        "# --- Assuming X and y DataFrames are already defined from previous steps ---\n",
        "# For demonstration purposes, let's create dummy X and y if they don't exist\n",
        "# In your actual code, these would come from your data splitting\n",
        "try:\n",
        "    # This block will attempt to use your existing X and y.\n",
        "    # If X or y are not defined, it will fall into the NameError,\n",
        "    # so we'll wrap it in a try-except or ensure they are defined upstream.\n",
        "    # If you are running this code block independently, you'll need to define X and y.\n",
        "    if 'X' not in locals() or 'y' not in locals():\n",
        "        print(\"X or y not found. Creating dummy data for demonstration.\")\n",
        "        # Create a dummy DataFrame if X and y are not already defined\n",
        "        data = {\n",
        "            'feature1': np.random.rand(100),\n",
        "            'feature2': np.random.randint(0, 5, 100),\n",
        "            'tenure': np.random.randint(1, 72, 100),\n",
        "            'TotalCharges': np.random.rand(100) * 1000\n",
        "        }\n",
        "        df_dummy = pd.DataFrame(data)\n",
        "        X = df_dummy[['feature1', 'feature2', 'tenure', 'TotalCharges']]\n",
        "        y = pd.Series(np.random.randint(0, 2, 100), name='Churn')\n",
        "\n",
        "except NameError:\n",
        "    print(\"X or y are not defined. Please ensure they are created from your dataset before running this section.\")\n",
        "    exit() # Exit if essential data is missing\n",
        "# --- End of dummy data creation (remove in your final script if X and y are properly defined) ---\n",
        "\n",
        "\n",
        "print(\"\\nGerando matriz de correla√ß√£o...\")\n",
        "plt.figure(figsize=(12, 10))\n",
        "# Concatena X e y para a matriz de correla√ß√£o, garantindo que 'Churn' esteja presente\n",
        "df_corr = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Calculando a matriz de correla√ß√£o apenas para vari√°veis num√©ricas\n",
        "numeric_cols = df_corr.select_dtypes(include=np.number).columns\n",
        "correlation_matrix = df_corr[numeric_cols].corr()\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Matriz de Correla√ß√£o das Vari√°veis')\n",
        "plt.show()\n",
        "\n",
        "if 'Churn' in correlation_matrix.columns:\n",
        "    print(\"\\nCorrela√ß√£o das vari√°veis com 'Churn':\")\n",
        "    print(correlation_matrix['Churn'].sort_values(ascending=False))\n",
        "\n",
        "\n",
        "# An√°lises Direcionadas: Investigue como vari√°veis espec√≠ficas se relacionam com a evas√£o.\n",
        "print(\"\\nAn√°lises direcionadas: Tempo de Contrato vs. Churn, Total Gasto vs. Churn\")\n",
        "\n",
        "# Verifique se as colunas existem antes de tentar plotar\n",
        "# Note: 'df' aqui se refere ao DataFrame original antes de separarmos X e y.\n",
        "# Se 'tenure' ou 'TotalCharges' foram transformadas (ex: OneHotEncoder), essa visualiza√ß√£o pode precisar ser ajustada\n",
        "# para usar os dados 'X' ou 'X_balanced' ap√≥s as transforma√ß√µes.\n",
        "# Assumindo que 'tenure' e 'TotalCharges' s√£o colunas num√©ricas originais.\n",
        "if 'tenure' in df_corr.columns and 'TotalCharges' in df_corr.columns:\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.boxplot(x='Churn', y='tenure', data=df_corr)\n",
        "    plt.title('Tempo de Contrato (tenure) vs. Evas√£o')\n",
        "    plt.ylabel('Tempo de Contrato (meses)')\n",
        "    plt.xlabel('Evas√£o (0=N√£o, 1=Sim)')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(x='Churn', y='TotalCharges', data=df_corr)\n",
        "    plt.title('Total Gasto (TotalCharges) vs. Evas√£o')\n",
        "    plt.ylabel('Total Gasto')\n",
        "    plt.xlabel('Evas√£o (0=N√£o, 1=Sim)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Colunas 'tenure' ou 'TotalCharges' n√£o encontradas para an√°lises direcionadas no DataFrame combinado. Verifique o nome das colunas.\")"
      ],
      "metadata": {
        "id": "stpspiZ6CPny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Normaliza√ß√£o ou Padroniza√ß√£o (se necess√°rio)"
      ],
      "metadata": {
        "id": "NodTlOMmCkXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "\n",
        "# --- Assuming X DataFrame is already defined from previous steps ---\n",
        "# For demonstration purposes, let's create a dummy X if it doesn't exist\n",
        "try:\n",
        "    if 'X' not in locals():\n",
        "        print(\"X not found. Creating dummy data for demonstration.\")\n",
        "        data = {\n",
        "            'numeric_feature_1': np.random.rand(100) * 100,\n",
        "            'numeric_feature_2': np.random.randint(10, 1000, 100),\n",
        "            'categorical_feature': ['A', 'B', 'A'] * 33 + ['B'],\n",
        "            'boolean_feature': [True, False] * 50\n",
        "        }\n",
        "        X = pd.DataFrame(data)\n",
        "        # Assuming you've already handled categorical encoding for X\n",
        "        # For this example, let's just use the numeric columns if they exist\n",
        "        X = X[['numeric_feature_1', 'numeric_feature_2']]\n",
        "\n",
        "except NameError:\n",
        "    print(\"X is not defined. Please ensure it is created from your dataset before running this section.\")\n",
        "    exit() # Exit if essential data is missing\n",
        "# --- End of dummy data creation (remove in your final script if X is properly defined) ---\n",
        "\n",
        "\n",
        "# Avalie a necessidade de normalizar ou padronizar os dados, conforme os modelos que ser√£o aplicados.\n",
        "\n",
        "# Identifica colunas num√©ricas para normaliza√ß√£o/padroniza√ß√£o\n",
        "colunas_numericas = X.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Cria uma c√≥pia dos dados para modelos com padroniza√ß√£o\n",
        "X_scaled = X.copy()\n",
        "if colunas_numericas:\n",
        "    X_scaled[colunas_numericas] = scaler.fit_transform(X[colunas_numericas])\n",
        "    print(\"\\nDados num√©ricos padronizados (StandardScaler) para modelos que exigem escala.\")\n",
        "else:\n",
        "    print(\"\\nNenhuma coluna num√©rica para padroniza√ß√£o ou j√° padronizadas.\")\n",
        "\n",
        "# X_unscaled j√° √© o 'X' original (ou balanceado, se SMOTE foi aplicado)\n",
        "X_unscaled = X\n",
        "\n",
        "print(\"\\nPrimeiras 5 linhas do DataFrame X_scaled (padronizado):\")\n",
        "print(X_scaled.head())\n",
        "print(\"\\nInforma√ß√µes sobre X_scaled:\")\n",
        "print(X_scaled.describe().loc[['mean', 'std']]) # Check mean (close to 0) and std (close to 1)"
      ],
      "metadata": {
        "id": "FB_a8GImCm7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Modelagem Preditiva"
      ],
      "metadata": {
        "id": "WgVetVbeCzuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa√ß√£o de Dados\n",
        "# Divida o conjunto de dados em treino e teste.\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "print(f\"\\nDados divididos em treino ({len(X_train_scaled)} amostras) e teste ({len(X_test_scaled)} amostras).\")\n",
        "\n",
        "# Garante que temos uma vers√£o n√£o-escalada para a √Årvore de Decis√£o, dividida de forma consistente\n",
        "X_train_unscaled, X_test_unscaled, _, _ = train_test_split(X_unscaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# Cria√ß√£o de Modelos\n",
        "print(\"\\n--- Modelagem Preditiva ---\")\n",
        "\n",
        "# Modelo 1: Regress√£o Log√≠stica (requer normaliza√ß√£o)\n",
        "print(\"\\nConstruindo Modelo de Regress√£o Log√≠stica (com dados padronizados)...\")\n",
        "model_lr = LogisticRegression(random_state=42, solver='liblinear', max_iter=1000) # Aumentado max_iter para converg√™ncia\n",
        "model_lr.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = model_lr.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nRelat√≥rio de Classifica√ß√£o - Regress√£o Log√≠stica:\")\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "print(\"\\nMatriz de Confus√£o - Regress√£o Log√≠stica:\")\n",
        "print(confusion_matrix(y_test, y_pred_lr))\n",
        "\n",
        "# Modelo 2: √Årvore de Decis√£o (n√£o requer normaliza√ß√£o)\n",
        "print(\"\\nConstruindo Modelo de √Årvore de Decis√£o (sem necessidade de padroniza√ß√£o)...\")\n",
        "model_dt = DecisionTreeClassifier(random_state=42)\n",
        "model_dt.fit(X_train_unscaled, y_train) # Usa os dados N√ÉO padronizados\n",
        "y_pred_dt = model_dt.predict(X_test_unscaled)\n",
        "\n",
        "print(\"\\nRelat√≥rio de Classifica√ß√£o - √Årvore de Decis√£o:\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "print(\"\\nMatriz de Confus√£o - √Årvore de Decis√£o:\")\n",
        "print(confusion_matrix(y_test, y_pred_dt))"
      ],
      "metadata": {
        "id": "9r_hIH03Gaoa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}